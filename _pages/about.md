---
permalink: /
title: About Me
excerpt: "Feng Li's Homepage"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I'm a first-year PhD student at the Hong Kong University of Science and Technology, advised by Prof. [Heung-Yeung Shum](https://scholar.google.com/citations?user=9akH-n8AAAAJ&hl=zh-CN) and Prof. [Lionel M. Ni](https://scholar.google.com/citations?user=OzMYwDIAAAAJ&hl=zh-CN). I am currently an intern at [International Digital Economy Academy (IDEA)](https://idea.edu.cn/), advised by Prof. [Lei Zhang](https://www.leizhang.org/).

**Research Interests**

* Computer Vision, Object Detection, Multi-modal learning


# üî• News
- [2022/3]: &nbsp;We release a survey Vision-Language Intelligence: Tasks, Representation Learning, and Large Models;
- [2022/3]: &nbsp;DN-DETR: Accelerate DETR Training by Introducing Query DeNoising is accepted by CVPR 2022 with score 112;

# üìù Publications 

<!-- <div class='paper-box'>
<div class='paper-box-text' markdown="1"> -->

* DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection.  
Hao Zhang\*, **Feng Li**\*, Shilong Liu\*, Lei Zhang, Hang Su, Jun Zhu, Lionel M. Ni, Heung-Yeung Shum.   
arxiv 2022.  
[[**Paper**]](https://arxiv.org/abs/2203.03605)[[**Code**]](https://github.com/IDEACVR/DINO)
  
* Vision-Language Intelligence: Tasks, Representation Learning, and Large Models.  
**Feng Li**\*, Hao Zhang\*, Yi-Fan Zhang, Shilong Liu, Jian Guo, Lionel M Ni, PengChuan Zhang, Lei Zhang.     
arxiv 2022.  
[[**Paper**]](https://arxiv.org/abs/2203.01922)      
  
* DN-DETR: Accelerate DETR Training by Introducing Query DeNoising.   
**Feng Li**\*, Hao Zhang\*, Shilong Liu, Jian Guo, Lionel M. Ni, Lei Zhang.   
IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2022.  
[[**Paper**]](https://arxiv.org/pdf/2203.01305)[[**Code**]](https://github.com/FengLi-ust/DN-DETR)
 
* DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR.   
Shilong Liu, **Feng Li**, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, Lei Zhang.    
International Conference on Learning Representations (ICLR) 2022.    
[[**Paper**]](https://arxiv.org/abs/2201.12329)[[**Code**]](https://github.com/SlongLiu/DAB-DETR)

* BiCrowd: Online Bi-Objective Incentive Mechanism for Mobile Crowd Sensing.   
Yi-Fan Zhang, Xinglin Zhang, and **Feng Li**. 
IEEE Internet of Things Journal (JCR Q1).  
[[**Paper**]](https://fengli-ust.github.io/files/BiCrowd-IOT-J.pdf)

<!-- </div>
</div> -->

_(* denotes equal contribution.)_
# üéñ Selected Awards
* Hong Kong Postgraduate Scholoarship, 2021
* Contemporary Undergraduate Mathematical Contest in Modeling(CUMCM), National first prize, 2019.

<!-- # üìñ Work experience
* March 2021 - Now: Research Assistant
  * Microsoft Research Asia, Beijing, China.
  * Duties included: 1. Design more powerful and simple object detection architecture based on the Transformer. 2. Understand NLP tasks such as NLI and exploit new paradigms to solve them more efficiently.
  * Advisor: Prof. [Jingdong Wang](https://jingdongwang2017.github.io/)

* August 2020 - Now: Research Assistant
  * University of Chinese Academy of Sciences, Beijing, China.
  * Duties included: 1. learning deep generative model for pedestrian generation. 2. cross-domain Re-ID from a causal view. 3. designing an efficient method to tackle problems in object detection and partial pedestrian re-identification.
  * Advisor: Prof. [Tieniu Tan](http://people.ucas.ac.cn/~tantieniu)
  * Co-Advisors: Prof. [Zhang Zhang](https://scholar.google.com/citations?user=rnRNwEMAAAAJ&hl=en) and Prof. [Liang Wang](https://scholar.google.com/citations?user=8kzzUboAAAAJ&hl=zh-CN)

* April 2018 ‚Äì July 2020: Research Assistant
  * South China University of Technology, Guangzhou, China.
  * Duties included: Incentive mechanism design for crowdsourcing platforms, edge computing
platforms, and federal learning platforms.
  * Advisor: Prof. Xinglin Zhang
 -->
<!-- # üí¨ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/)

# üíª Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China. -->
